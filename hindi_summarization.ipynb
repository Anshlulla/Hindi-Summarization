{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9614754,
          "sourceType": "datasetVersion",
          "datasetId": 5867330
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anshlulla/Hindi-Summarization/blob/main/hindi_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge -qq"
      ],
      "metadata": {
        "id": "U-f1BA7Chamr",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:32:52.476340Z",
          "iopub.execute_input": "2024-10-17T13:32:52.477087Z",
          "iopub.status.idle": "2024-10-17T13:33:04.958717Z",
          "shell.execute_reply.started": "2024-10-17T13:32:52.477045Z",
          "shell.execute_reply": "2024-10-17T13:33:04.957411Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "import os\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ],
      "metadata": {
        "id": "_WuxAHxGY_6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f52109-f4fb-4a7d-c913-0633d3729d7f",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:33:06.650262Z",
          "iopub.execute_input": "2024-10-17T13:33:06.651250Z",
          "iopub.status.idle": "2024-10-17T13:33:07.625170Z",
          "shell.execute_reply.started": "2024-10-17T13:33:06.651190Z",
          "shell.execute_reply": "2024-10-17T13:33:07.624238Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the BiLSTM model\n",
        "class BiLSTMSummarizer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMSummarizer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
        "\n",
        "        embedded = self.embedding(src)\n",
        "        enc_output, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            input_embedded = self.embedding(input).unsqueeze(1)\n",
        "            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))\n",
        "            prediction = self.fc(output.squeeze(1))\n",
        "            outputs[:, t] = prediction\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "mCIhYCOdY_6E",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:33:08.248392Z",
          "iopub.execute_input": "2024-10-17T13:33:08.248785Z",
          "iopub.status.idle": "2024-10-17T13:33:08.260632Z",
          "shell.execute_reply.started": "2024-10-17T13:33:08.248750Z",
          "shell.execute_reply": "2024-10-17T13:33:08.259577Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles, summaries, vocab, max_length=100):\n",
        "        self.articles = articles\n",
        "        self.summaries = summaries\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "\n",
        "        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n",
        "        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n",
        "\n",
        "        return torch.tensor(article_indices), torch.tensor(summary_indices)"
      ],
      "metadata": {
        "id": "gBEvTN3KY_6F",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:33:09.100768Z",
          "iopub.execute_input": "2024-10-17T13:33:09.101483Z",
          "iopub.status.idle": "2024-10-17T13:33:09.110469Z",
          "shell.execute_reply.started": "2024-10-17T13:33:09.101442Z",
          "shell.execute_reply": "2024-10-17T13:33:09.109559Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df['Content'].tolist(), df['Headline'].tolist()\n",
        "\n",
        "# Tokenize text\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(texts, min_freq=2):\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab, {v: k for k, v in vocab.items()}"
      ],
      "metadata": {
        "id": "tgn_dCLbY_6H",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:33:09.668767Z",
          "iopub.execute_input": "2024-10-17T13:33:09.669555Z",
          "iopub.status.idle": "2024-10-17T13:33:09.676680Z",
          "shell.execute_reply.started": "2024-10-17T13:33:09.669514Z",
          "shell.execute_reply": "2024-10-17T13:33:09.675673Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "articles, summaries = load_data('/kaggle/input/hindi-news-dataset/hindi_news_dataset.csv')\n",
        "\n",
        "# Tokenize data\n",
        "tokenized_articles = [tokenize(article) for article in articles]\n",
        "tokenized_summaries = [tokenize(summary) for summary in summaries]\n",
        "\n",
        "# Build vocabulary\n",
        "vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)\n",
        "\n",
        "# Split data\n",
        "train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n",
        "train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "lHfREZHwY_6I",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:33:10.922303Z",
          "iopub.execute_input": "2024-10-17T13:33:10.923160Z",
          "iopub.status.idle": "2024-10-17T13:35:45.623152Z",
          "shell.execute_reply.started": "2024-10-17T13:33:10.923099Z",
          "shell.execute_reply": "2024-10-17T13:35:45.622287Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = SummarizationDataset(train_articles, train_summaries, vocab)\n",
        "val_dataset = SummarizationDataset(val_articles, val_summaries, vocab)\n",
        "test_dataset = SummarizationDataset(test_articles, test_summaries, vocab)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "CP18TCZfY_6J",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:35:45.625254Z",
          "iopub.execute_input": "2024-10-17T13:35:45.625671Z",
          "iopub.status.idle": "2024-10-17T13:35:45.632196Z",
          "shell.execute_reply.started": "2024-10-17T13:35:45.625626Z",
          "shell.execute_reply": "2024-10-17T13:35:45.631255Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BiLSTMSummarizer(len(vocab), 100, 128, len(vocab)).to(device)\n",
        "device"
      ],
      "metadata": {
        "id": "-X7pLGHsY_6J",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:35:45.633344Z",
          "iopub.execute_input": "2024-10-17T13:35:45.633707Z",
          "iopub.status.idle": "2024-10-17T13:35:46.183189Z",
          "shell.execute_reply.started": "2024-10-17T13:35:45.633670Z",
          "shell.execute_reply": "2024-10-17T13:35:46.182220Z"
        },
        "trusted": true,
        "outputId": "356ede25-9688-4efd-c573-33764b33edec"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train function\n",
        "def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(iterator, desc=\"Training\"):\n",
        "        src, trg = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "UZGJGt5-Y_6K",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:35:53.617477Z",
          "iopub.execute_input": "2024-10-17T13:35:53.617889Z",
          "iopub.status.idle": "2024-10-17T13:35:53.626239Z",
          "shell.execute_reply.started": "2024-10-17T13:35:53.617851Z",
          "shell.execute_reply": "2024-10-17T13:35:53.625017Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src, trg = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0)  # turn off teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "sUtmmZLIY_6L",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:35:54.075102Z",
          "iopub.execute_input": "2024-10-17T13:35:54.076071Z",
          "iopub.status.idle": "2024-10-17T13:35:54.084970Z",
          "shell.execute_reply.started": "2024-10-17T13:35:54.076016Z",
          "shell.execute_reply": "2024-10-17T13:35:54.083930Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=100, min_length=10, device='cpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Embedding the input sequence\n",
        "        embedded = model.embedding(src)  # shape: (batch_size, seq_len, embedding_dim)\n",
        "        enc_output, (hidden, cell) = model.encoder(embedded)  # LSTM encoder output\n",
        "\n",
        "        # In case of bi-directional LSTM, combine the hidden states\n",
        "        if model.encoder.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # shape: (batch_size, hidden_dim)\n",
        "            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)        # shape: (batch_size, hidden_dim)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]  # Take the last layer if not bi-directional\n",
        "            cell = cell[-1, :, :]      # Take the last layer if not bi-directional\n",
        "\n",
        "        # Now we process one sequence at a time, so set batch size to 1\n",
        "        hidden = hidden.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
        "        cell = cell.unsqueeze(0)      # shape: (1, batch_size, hidden_dim)\n",
        "\n",
        "        # Initialize the beam with the start-of-sequence token\n",
        "        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]  # Start with one sequence\n",
        "        complete_hypotheses = []\n",
        "\n",
        "        # Perform beam search\n",
        "        for t in range(max_length):\n",
        "            new_beam = []\n",
        "            for seq, score, hidden, cell in beam:\n",
        "                # If end-of-sequence token is reached and length is >= min_length, add to complete hypotheses\n",
        "                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n",
        "                    complete_hypotheses.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Prepare the input for the decoder (last predicted token)\n",
        "                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)  # shape: (1, 1)\n",
        "                input_embedded = model.embedding(input)  # shape: (1, 1, embedding_dim)\n",
        "\n",
        "                # Pass through the decoder with the current hidden and cell states\n",
        "                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))  # hidden, cell are (1, 1, hidden_dim)\n",
        "                predictions = model.fc(output.squeeze(1))  # shape: (1, vocab_size)\n",
        "\n",
        "                # Prevent EOS if sequence is shorter than minimum length\n",
        "                if len(seq) < min_length:\n",
        "                    predictions[0][vocab['<eos>']] = float('-inf')\n",
        "\n",
        "                # Get top beam_width predictions\n",
        "                top_preds = torch.topk(predictions, beam_width, dim=1)\n",
        "\n",
        "                # For each top prediction, extend the sequence and update the beam\n",
        "                for i in range(beam_width):\n",
        "                    new_seq = seq + [top_preds.indices[0][i].item()]\n",
        "                    new_score = score - top_preds.values[0][i].item()  # Negative log probability\n",
        "                    new_hidden = hidden.clone()\n",
        "                    new_cell = cell.clone()\n",
        "                    new_beam.append((new_seq, new_score, new_hidden, new_cell))\n",
        "\n",
        "            # Sort by score and keep top beam_width sequences\n",
        "            beam = sorted(new_beam, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "            if len(complete_hypotheses) >= beam_width:\n",
        "                break\n",
        "\n",
        "        # Sort and return the best sequence\n",
        "        complete_hypotheses = sorted(complete_hypotheses, key=lambda x: x[1])\n",
        "        if complete_hypotheses:\n",
        "            best_seq = complete_hypotheses[0][0]\n",
        "        else:\n",
        "            best_seq = beam[0][0]\n",
        "\n",
        "    # Convert sequence of indices back to words\n",
        "    return [inv_vocab[idx] for idx in best_seq if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]"
      ],
      "metadata": {
        "id": "dFZaxsetY_6L",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:35:55.329764Z",
          "iopub.execute_input": "2024-10-17T13:35:55.330123Z",
          "iopub.status.idle": "2024-10-17T13:35:55.348004Z",
          "shell.execute_reply.started": "2024-10-17T13:35:55.330090Z",
          "shell.execute_reply": "2024-10-17T13:35:55.346797Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model function\n",
        "def save_model(model, vocab, filepath, best_val_loss):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab': vocab,\n",
        "        'best_val_loss': best_val_loss # Save best_val_loss\n",
        "    }, filepath)\n",
        "    print(f\"Model saved to {filepath}\")"
      ],
      "metadata": {
        "id": "iYMa8KYbY_6M",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:35:56.423540Z",
          "iopub.execute_input": "2024-10-17T13:35:56.424505Z",
          "iopub.status.idle": "2024-10-17T13:35:56.430531Z",
          "shell.execute_reply.started": "2024-10-17T13:35:56.424449Z",
          "shell.execute_reply": "2024-10-17T13:35:56.429248Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
        "\n",
        "# Clear unused GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Define learning rate scheduler (CosineAnnealingLR)\n",
        "# It will reduce the learning rate when validation loss stops improving\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=2, verbose=True)"
      ],
      "metadata": {
        "id": "fCI_lkWhY_6M",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:35:56.654290Z",
          "iopub.execute_input": "2024-10-17T13:35:56.655155Z",
          "iopub.status.idle": "2024-10-17T13:35:57.521261Z",
          "shell.execute_reply.started": "2024-10-17T13:35:56.655104Z",
          "shell.execute_reply": "2024-10-17T13:35:57.520339Z"
        },
        "trusted": true,
        "outputId": "084ef6fd-881a-4c4c-d79c-9da2e083d0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "# 1st epoch\n",
        "num_epochs = 1\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')\n",
        "\n",
        "    # Step the scheduler based on the validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_model(model, vocab, 'best_model.pth', best_val_loss)\n",
        "        torch.cuda.empty_cache() # Clear unused cache to free up GPU memory"
      ],
      "metadata": {
        "id": "WyyssAHKY_6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e6a7b4c-ad01-4bd5-9f62-0660ec482239",
        "execution": {
          "iopub.status.busy": "2024-10-17T13:42:33.718871Z",
          "iopub.execute_input": "2024-10-17T13:42:33.720032Z",
          "iopub.status.idle": "2024-10-17T14:54:59.100920Z",
          "shell.execute_reply.started": "2024-10-17T13:42:33.719989Z",
          "shell.execute_reply": "2024-10-17T14:54:59.099937Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 4174/4174 [1:08:19<00:00,  1.02it/s]\nEvaluating: 100%|██████████| 464/464 [04:05<00:00,  1.89it/s]\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 01\n\tTrain Loss: 6.030\n\t Val. Loss: 5.695\nModel saved to best_model.pth\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model function\n",
        "def load_model(filepath, device):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    vocab = checkpoint['vocab']\n",
        "    best_val_loss = checkpoint['best_val_loss'] # Load best_val_loss\n",
        "    model = BiLSTMSummarizer(len(vocab), 100, 128, len(vocab)).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, checkpoint, best_val_loss # Return best_val_loss"
      ],
      "metadata": {
        "id": "3Di3pobeY_6N",
        "execution": {
          "iopub.status.busy": "2024-10-17T16:46:45.822788Z",
          "iopub.execute_input": "2024-10-17T16:46:45.823173Z",
          "iopub.status.idle": "2024-10-17T16:46:45.828909Z",
          "shell.execute_reply.started": "2024-10-17T16:46:45.823137Z",
          "shell.execute_reply": "2024-10-17T16:46:45.827881Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model after 1st epoch\n",
        "best_model, _, best_val_loss = load_model('/kaggle/working/best_model.pth', device)\n",
        "\n",
        "# Training for 2nd and 3rd epochs\n",
        "num_epochs = 2\n",
        "\n",
        "# Train for the second epoch using the saved model\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(best_model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(best_model, val_loader, criterion, device)\n",
        "\n",
        "    # Print train and validation loss\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')\n",
        "\n",
        "    # Save the model if the validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss  # Update the best validation loss\n",
        "        save_model(best_model, vocab, 'best_model.pth', best_val_loss)  # Save model\n",
        "        torch.cuda.empty_cache() # Clear unused cache to free up GPU memory\n",
        "\n",
        "# After training is complete\n",
        "torch.cuda.empty_cache()  # Clear GPU cache one last time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "_7855ODHYvmJ",
        "outputId": "895924a0-98b5-48ab-bd01-ff9b21e28e1c",
        "execution": {
          "iopub.status.busy": "2024-10-17T16:46:50.681645Z",
          "iopub.execute_input": "2024-10-17T16:46:50.682479Z",
          "iopub.status.idle": "2024-10-17T19:11:02.433838Z",
          "shell.execute_reply.started": "2024-10-17T16:46:50.682442Z",
          "shell.execute_reply": "2024-10-17T19:11:02.432906Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_30/2281548745.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filepath, map_location=device)\nTraining: 100%|██████████| 4174/4174 [1:07:59<00:00,  1.02it/s]\nEvaluating: 100%|██████████| 464/464 [04:06<00:00,  1.88it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 01\n\tTrain Loss: 5.058\n\t Val. Loss: 5.695\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 4174/4174 [1:08:00<00:00,  1.02it/s]\nEvaluating: 100%|██████████| 464/464 [04:04<00:00,  1.90it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 02\n\tTrain Loss: 5.060\n\t Val. Loss: 5.695\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model for testing\n",
        "best_model, _, _ = load_model('best_model.pth', device)\n",
        "\n",
        "# Test the model\n",
        "test_loss = evaluate(best_model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Evaluate using ROUGE score\n",
        "rouge = Rouge()\n",
        "best_model.eval()\n",
        "predictions = []\n",
        "references = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
        "        src, trg = batch\n",
        "        src = src.to(device)\n",
        "        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)  # Set minimum length\n",
        "        predictions.extend([' '.join(pred)])\n",
        "        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n",
        "\n",
        "# Ensure all predictions meet the minimum length\n",
        "min_length = 10  # Set this to your desired minimum length\n",
        "predictions = [p if len(p.split()) >= min_length else p + ' ' + ' '.join(['<pad>'] * (min_length - len(p.split()))) for p in predictions]\n",
        "\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "print(\"ROUGE scores:\")\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "DPyf-nlSY_6N",
        "outputId": "42919016-1115-47fc-ab0f-223bc5d20ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2024-10-17T19:11:41.564853Z",
          "iopub.execute_input": "2024-10-17T19:11:41.565686Z",
          "iopub.status.idle": "2024-10-17T19:22:42.793020Z",
          "shell.execute_reply.started": "2024-10-17T19:11:41.565645Z",
          "shell.execute_reply": "2024-10-17T19:22:42.792063Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_30/2281548745.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filepath, map_location=device)\nEvaluating: 100%|██████████| 1160/1160 [10:13<00:00,  1.89it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 5.700\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Generating summaries: 100%|██████████| 1160/1160 [00:46<00:00, 24.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ROUGE scores:\n{'rouge-1': {'r': 0.23247931357899754, 'p': 0.4455709328985187, 'f': 0.29887501308560693}, 'rouge-2': {'r': 0.06618892052101748, 'p': 0.09416376726721573, 'f': 0.07608365108299221}, 'rouge-l': {'r': 0.2067549893741905, 'p': 0.3964412933809486, 'f': 0.2657179110762273}}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading pre-trained model...\")\n",
        "trained_model, checkpoint, _ = load_model('best_model.pth', device)\n",
        "vocab = checkpoint['vocab']\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "trained_model = trained_model.to(device)"
      ],
      "metadata": {
        "id": "Y68PKEeYY_6N",
        "outputId": "92d0777f-cd84-4b63-efb5-8a0e8a4b2447",
        "execution": {
          "iopub.status.busy": "2024-10-17T19:23:49.755556Z",
          "iopub.execute_input": "2024-10-17T19:23:49.755957Z",
          "iopub.status.idle": "2024-10-17T19:23:50.115027Z",
          "shell.execute_reply.started": "2024-10-17T19:23:49.755918Z",
          "shell.execute_reply": "2024-10-17T19:23:50.114037Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading pre-trained model...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_30/2281548745.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filepath, map_location=device)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified Summarization bot\n",
        "def summarize_text(model, vocab, inv_vocab, text, max_length=100, min_length=10, beam_width=3, device='cpu', debug=False):\n",
        "    model.eval()\n",
        "    tokens = tokenize(text)[:max_length]\n",
        "    indices = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n",
        "    src = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    summary = beam_search(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\n",
        "\n",
        "    if debug:\n",
        "        print(\"Input tokens:\", tokens)\n",
        "        print(\"Input indices:\", indices)\n",
        "        print(\"Generated indices:\", [vocab[word] for word in summary])\n",
        "        print(\"Summary length:\", len(summary))\n",
        "\n",
        "    return ' '.join(summary)"
      ],
      "metadata": {
        "id": "CMv-l-VmY_6O",
        "execution": {
          "iopub.status.busy": "2024-10-17T19:24:06.511141Z",
          "iopub.execute_input": "2024-10-17T19:24:06.511511Z",
          "iopub.status.idle": "2024-10-17T19:24:06.519104Z",
          "shell.execute_reply.started": "2024-10-17T19:24:06.511478Z",
          "shell.execute_reply": "2024-10-17T19:24:06.518185Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the summarization bot\n",
        "input_text = \"ऑस्ट्रेलिया ने ब्लूमफोनटीन में पहले वनडे में दक्षिण अफ्रीका को 3-विकेट से हरा दिया। यह 12 वर्षों में दक्षिण अफ्रीका के खिलाफ उसकी धरती पर ऑस्ट्रेलिया की पहली वनडे जीत है। ऑस्ट्रेलिया का स्कोर 16.3 ओवर में 113/7 था लेकिन मार्नस लबुशेन और ऐश्टन एगर की 112* रनों की साझेदारी की बदौलत उसने 40.2 ओवर में लक्ष्य हासिल कर लिया।\"\n",
        "summary = summarize_text(trained_model, vocab, inv_vocab, input_text, min_length=10, device=device, debug=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)\n",
        "print(\"Summary length:\", len(summary.split()))"
      ],
      "metadata": {
        "id": "-6HLvvrkY_6O",
        "outputId": "b4210c5f-ccef-496b-9339-27f7b8ec863a",
        "execution": {
          "iopub.status.busy": "2024-10-17T19:24:28.675824Z",
          "iopub.execute_input": "2024-10-17T19:24:28.676528Z",
          "iopub.status.idle": "2024-10-17T19:24:28.724010Z",
          "shell.execute_reply.started": "2024-10-17T19:24:28.676492Z",
          "shell.execute_reply": "2024-10-17T19:24:28.723119Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Input tokens: ['ऑस्ट्रेलिया', 'ने', 'ब्लूमफोनटीन', 'में', 'पहले', 'वनडे', 'में', 'दक्षिण', 'अफ्रीका', 'को', '3-विकेट', 'से', 'हरा', 'दिया।', 'यह', '12', 'वर्षों', 'में', 'दक्षिण', 'अफ्रीका', 'के', 'खिलाफ', 'उसकी', 'धरती', 'पर', 'ऑस्ट्रेलिया', 'की', 'पहली', 'वनडे', 'जीत', 'है।', 'ऑस्ट्रेलिया', 'का', 'स्कोर', '16.3', 'ओवर', 'में', '113/7', 'था', 'लेकिन', 'मार्नस', 'लबुशेन', 'और', 'ऐश्टन', 'एगर', 'की', '112*', 'रनों', 'की', 'साझेदारी', 'की', 'बदौलत', 'उसने', '40.2', 'ओवर', 'में', 'लक्ष्य', 'हासिल', 'कर', 'लिया।']\nInput indices: [2, 4910, 37, 8412, 14, 193, 7443, 14, 2425, 2426, 10, 8413, 86, 7167, 96, 67, 1330, 1254, 14, 2425, 2426, 12, 189, 274, 8414, 50, 4910, 8, 725, 7443, 2940, 35, 4910, 71, 7770, 8415, 7479, 14, 8416, 428, 596, 7627, 7628, 43, 8417, 8418, 8, 8419, 7930, 8, 3882, 8, 7536, 434, 8420, 7479, 14, 4471, 4275, 20, 1960, 3]\nGenerated indices: [237, 37, 7443, 1249, 7458, 3979, 14, 237, 12, 183, 237, 12, 93, 15120, 86, 7751]\nSummary length: 16\nGenerated Summary:\nभारत ने वनडे विश्व कप इतिहास में भारत के लिए भारत के बाद कैसी से हराया\nSummary length: 16\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}